= Lab 4: Deploy application with IRSA

== Learning objectives

By the end of this lab, you will be able to:

* Create and configure AWS DynamoDB tables for cloud-native applications
* Implement IAM Roles for Service Accounts (IRSA) to eliminate static credentials
* Configure OIDC trust policies for secure AWS service access from ROSA
* Deploy Java applications using Quarkus and Source-to-Image (S2I) build strategy
* Understand OpenShift BuildConfigs, ImageStreams, and Deployments
* Expose applications using OpenShift Routes for external access

== Value proposition

*For field teams*: This lab demonstrates ROSA's most critical security capability ‚Äì eliminating static credentials through IRSA. This is the #1 security concern customers raise about Kubernetes-AWS integration. Use this to show how ROSA solves credential management better than self-managed Kubernetes.

*Customer proof points*:

* *Security incident reduction*: IRSA eliminates static credentials, addressing the fact that access credentials are the first target attackers seek^1^
* *Compliance simplification*: Temporary credentials automatically rotate, eliminating manual credential rotation processes and reducing audit scope for compliance
* *Developer productivity*: Applications access AWS services without managing credentials or handling credential logic in application code
* *Zero trust architecture*: Pod-level identity with least-privilege access enables true zero trust security implementations

^1^ link:https://www.cncf.io/blog/2025/04/22/these-kubernetes-mistakes-will-make-you-an-easy-target-for-hackers/[CNCF: Kubernetes Security Best Practices^] - Credentials are primary attack targets

== Introduction

It's time for us to put our cluster to work and deploy a workload! We're going to build an example Java application, https://github.com/redhat-mw-demos/microsweeper-quarkus/tree/ROSA[microsweeper,window=_blank], using https://quarkus.io/[Quarkus,window=_blank] (a Kubernetes-native Java stack) and https://aws.amazon.com/dynamodb[Amazon DynamoDB,window=_blank]. We'll then deploy the application to our ROSA cluster and connect to the database over AWS's secure network.

This lab demonstrates how ROSA (an AWS native service) can easily and securely access and utilize other AWS native services using AWS Secure Token Service (STS). To achieve this, we will be using AWS IAM, Amazon DynamoDB, and a service account within OpenShift. After configuring the latter, we will use both Quarkus - a Kubernetes-native Java framework optimized for containers - and Source-to-Image (S2I) - a toolkit for building container images from source code - to deploy the microsweeper application.

== Create an Amazon DynamoDB instance

. First, let's create a project (also known as a namespace). A project is a unit of organization within OpenShift that provides isolation for applications and resources. To do so, run the following command:
+
[source,sh,role=execute]
----
oc new-project microsweeper-ex
----
+
.Sample Output
[source,text,options=nowrap]
----
Now using project "microsweeper-ex" on server "https://api.rosa-6n4s8.1c1c.p1.openshiftapps.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app rails-postgresql-example

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 -- /agnhost serve-hostname
----

. Next, create the Amazon DynamoDB table resource. Amazon DynamoDB will be used to store information from our application and ROSA will utilize AWS Secure Token Service (STS) to access this native service. More information on STS and how it is utilized in ROSA will be provided in the next section. For now let's create the Amazon DynamoDB table, To do so, run the following command:
+
[source,sh,role=execute]
----
aws dynamodb create-table \
  --table-name microsweeper-scores-${GUID} \
  --attribute-definitions AttributeName=name,AttributeType=S \
  --key-schema AttributeName=name,KeyType=HASH \
  --provisioned-throughput ReadCapacityUnits=1,WriteCapacityUnits=1
----
+
.Sample Output
[source,json,options=nowrap]
----
{
    "TableDescription": {
        "AttributeDefinitions": [
            {
                "AttributeName": "name",
                "AttributeType": "S"
            }
        ],
        "TableName": "microsweeper-scores-6n4s8",
        "KeySchema": [
            {
                "AttributeName": "name",
                "KeyType": "HASH"
            }
        ],
        "TableStatus": "CREATING",
        "CreationDateTime": 1681832377.864,
        "ProvisionedThroughput": {
            "NumberOfDecreasesToday": 0,
            "ReadCapacityUnits": 1,
            "WriteCapacityUnits": 1
        },
        "TableSizeBytes": 0,
        "ItemCount": 0,
        "TableArn": "arn:aws:dynamodb:us-east-2:264091519843:table/microsweeper-scores-6n4s8",
        "TableId": "37be72fe-3dea-411c-871d-467c12607691"
    }
}
----

== IAM Roles for Service Account (IRSA) Configuration

Our application uses AWS Secure Token Service(STS) to establish connections with Amazon DynamoDB. Traditionally, one would use static IAM credentials for this purpose, but this approach goes against AWS' recommended best practices. Instead, AWS suggests utilizing their Secure Token Service (STS). Fortunately, our ROSA cluster has already been deployed using AWS STS, making it effortless to adopt IAM Roles for Service Accounts (IRSA), also known as pod identity.

Service accounts play a crucial role in managing the permissions and access control of applications running within ROSA. They act as identities for pods and allow them to interact securely with various AWS services.

IAM roles, on the other hand, define a set of permissions that can be assumed by trusted entities within AWS. By associating an AWS IAM role with a service account, we enable the pods in our ROSA cluster to leverage the permissions defined within that role. This means that instead of relying on static IAM credentials, our application can obtain temporary security tokens from AWS STS by assuming the associated IAM role.

This approach aligns with AWS' recommended best practices and provides several benefits. Firstly, it enhances security by reducing the risk associated with long-lived static credentials. Secondly, it simplifies the management of access controls by leveraging IAM roles, which can be centrally managed and easily updated. Finally, it enables seamless integration with AWS services, such as DynamoDB, by granting the necessary permissions to the service accounts associated with our pods.

image::/irsa-sts.jpeg[IRSA STS Architecture Diagram,link=/irsa-sts.jpeg,window=_blank]

. First, create a service account to use to assume an IAM role.
To do so, run the following command:
+
[source,sh,role=execute]
----
oc -n microsweeper-ex create serviceaccount microsweeper
----
+
.Sample Output
[source,text,options=nowrap]
----
serviceaccount/microsweeper created
----

. Next, let's create a trust policy document which will define what service account can assume our role.
To create the trust policy document, run the following command:
+
[source,sh,role=execute]
----
cat <<EOF > ${HOME}/trust-policy.json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Federated": "arn:aws:iam::$(aws sts get-caller-identity --query 'Account' --output text):oidc-provider/$(rosa describe cluster -c rosa-${GUID} -o json | jq -r .aws.sts.oidc_endpoint_url | sed -e 's/^https:\/\///')"
      },
      "Action": "sts:AssumeRoleWithWebIdentity",
      "Condition": {
        "StringEquals": {
          "$(rosa describe cluster -c rosa-${GUID} -o json | jq -r .aws.sts.oidc_endpoint_url | sed -e 's/^https:\/\///'):sub": "system:serviceaccount:microsweeper-ex:microsweeper"
        }
      }
    }
  ]
}
EOF
----

. Next, let's take the trust policy document and use it to create a role.
To do so, run the following command:
+
[source,sh,role=execute]
----
aws iam create-role --role-name irsa-${GUID} --assume-role-policy-document file://${HOME}/trust-policy.json --description "IRSA Role (${GUID}"
----
+
.Sample Output
[source,text,options=nowrap]
----
{
    "Role": {
        "Path": "/",
        "RoleName": "irsa_6n4s8",
        "RoleId": "AROAT27IUZNRSSYVO24ET",
        "Arn": "arn:aws:iam::264091519843:role/irsa_6n4s8",
        "CreateDate": "2023-04-18T18:15:48Z",
        "AssumeRolePolicyDocument": {
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Effect": "Allow",
                    "Principal": {
                        "Federated": "arn:aws:iam::264091519843:oidc-provider/rh-oidc.s3.us-east-1.amazonaws.com/235ftpmaq3oavfin8mt600af4sar9oej"
                    },
                    "Action": "sts:AssumeRoleWithWebIdentity",
                    "Condition": {
                        "StringEquals": {
                            "rh-oidc.s3.us-east-1.amazonaws.com/235ftpmaq3oavfin8mt600af4sar9oej:sub": "system:serviceaccount:microsweeper-ex:microsweeper"
                        }
                    }
                }
            ]
        }
    }
}
----

. Next, let's attach the `AmazonDynamoDBFullAccess` policy to our newly created IAM role.
This will allow our application to read and write to our Amazon DynamoDB table.
To do so, run the following command:
+
[source,sh,role=execute]
----
aws iam attach-role-policy --role-name irsa-${GUID} --policy-arn=arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess
----

. Finally, let's annotate the service account with the ARN of the IAM role we created above.
To do so, run the following command:
+
[source,sh,role=execute]
----
oc -n microsweeper-ex annotate serviceaccount microsweeper eks.amazonaws.com/role-arn=arn:aws:iam::$(aws sts get-caller-identity --query 'Account' --output text):role/irsa-${GUID}
----
+
.Sample Output
[source,text,options=nowrap]
----
serviceaccount/microsweeper annotated
----

== Build and deploy the Microsweeper app

Now that we've got a DynamoDB instance up and running and our IRSA configuration completed, let's build and deploy our application.

. In order to build the application you will need the Java JDK 17 and the Quarkus CLI installed. Java JDK 17 is already installed on your bastion VM so let's install the Quarkus CLI:
+
[source,sh,role=execute]
----
curl -Ls https://sh.jbang.dev | bash -s - trust add https://repo1.maven.org/maven2/io/quarkus/quarkus-cli/
curl -Ls https://sh.jbang.dev | bash -s - app install --fresh --force quarkus@quarkusio

echo "export JAVA_HOME=/usr/lib/jvm/jre-17-openjdk" >>${HOME}/.bashrc
echo "export PATH=\$JAVA_HOME/bin:\$PATH" >>${HOME}/.bashrc

source ${HOME}/.bashrc
----

. Double check the Quarkus CLI version:
+
[source,sh,role=execute]
----
quarkus --version
----
+
.Sample Output
[source,text,options=nowrap]
----
3.30.6
----

. Now, let's clone the application from GitHub.
To do so, run the following command:
+
[source,sh,role=execute]
----
cd ${HOME}

git clone https://github.com/rh-mobb/rosa-workshop-app.git
----

. Next, let's change directory into the newly cloned Git repository.
To do so, run the following command:
+
[source,sh,role=execute]
----
cd ${HOME}/rosa-workshop-app
----

. Next, we will add the OpenShift extension to the Quarkus CLI.
To do so, run the following command:
+
[source,sh,role=execute]
----
quarkus ext add openshift
----
+
.Sample Output
[source,text,options=nowrap]
----
Looking for the newly published extensions in registry.quarkus.io
 üëç  Extension io.quarkus:quarkus-openshift was already installed
----

. Now, we'll configure Quarkus to use the DynamoDB instance that we created earlier in this section.
To do so, we'll create an `application.properties` file using by running the following command:
+
[source,sh,role=execute]
----
cat <<EOF > ${HOME}/rosa-workshop-app/src/main/resources/application.properties
# AWS DynamoDB configurations
%dev.quarkus.dynamodb.endpoint-override=http://localhost:8000
%prod.quarkus.openshift.env.vars.aws_region=$(aws configure get region)
%prod.quarkus.dynamodb.aws.credentials.type=default
dynamodb.table=microsweeper-scores-${GUID}

# OpenShift configurations
%prod.quarkus.kubernetes-client.trust-certs=true
%prod.quarkus.kubernetes.deploy=true
%prod.quarkus.kubernetes.deployment-target=openshift
%prod.quarkus.openshift.build-strategy=docker
%prod.quarkus.openshift.route.expose=true
%prod.quarkus.openshift.service-account=microsweeper

# To make Quarkus use Deployment instead of DeploymentConfig
%prod.quarkus.openshift.deployment-kind=Deployment
%prod.quarkus.container-image.group=microsweeper-ex
EOF
----

. Now that we've provided the proper configuration, we will build our application.
We'll do this using https://github.com/openshift/source-to-image[source-to-image,window=_blank], a tool built-in to OpenShift.
To start the build and deploy, run the following command:
+
[source,sh,role=execute]
----
quarkus build --no-tests
----
+
.Sample Output
[source,text,options=nowrap]
----
[...Lots of Output Omitted...]
[INFO] Installing /home/rosa/rosa-workshop-app/target/microsweeper-appservice-1.0.0-SNAPSHOT.jar to /home/rosa/.m2/repository/org/acme/microsweeper-appservice/1.0.0-SNAPSHOT/microsweeper-appservice-1.0.0-SNAPSHOT.jar
[INFO] Installing /home/rosa/rosa-workshop-app/pom.xml to /home/rosa/.m2/repository/org/acme/microsweeper-appservice/1.0.0-SNAPSHOT/microsweeper-appservice-1.0.0-SNAPSHOT.pom
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 02:02 min
[INFO] Finished at: 2023-04-18T18:32:26Z
[INFO] ------------------------------------------------------------------------
----

== Review

Let's take a look at what this command did, along with everything that was created in your cluster.

Return to your OpenShift Web Console.

**OpenShift Web Console Access:**

[cols="1,2"]
|===
| *Console URL* | `{rosa_openshift_console_url}`
| *Username* | `{rosa_openshift_admin_user}`
| *Password* | `{rosa_openshift_admin_password}`
|===

[TIP]
====
You can also get the console URL by running: `oc whoami --show-console`
====

=== Container Images

From the Administrator perspective, expand _Builds_ and then _ImageStreams_, and select the _microsweeper-ex_ project.

image::rosa-console-imagestreams.png[OpenShift Web Console - Imagestreams,link=rosa-console-imagestreams.png,window=_blank]

You will see two images that were created on your behalf when you ran the quarkus build command.
There is one image for `openjdk-17` that comes with OpenShift as a Universal Base Image (UBI) that the application will run under.
With UBI, you get highly optimized and secure container images that you can build your applications with.
For more information on UBI please visit the UBI section of the https://catalog.redhat.com/software/base-images[Red Hat Ecosystem Catalog,window=_blank].

The second image you see is the the `microsweeper-appservice` image.
This is the image for the application that was built automatically for you and pushed to the built-in container registry inside of OpenShift.

=== Image Build

How did those images get built you ask?
Back on the OpenShift Web Console, click on _BuildConfigs_ and then the _microsweeper-appservice_ entry.

image::rosa-console-buildconfigs.png[OpenShift Web Console - BuildConfigs,link=rosa-console-buildconfigs.png,window=_blank]
image::rosa-console-microsweeper-appservice-buildconfig.png[OpenShift Web Console - microsweeper-appservice BuildConfig,link=rosa-console-microsweeper-appservice-buildconfig.png,window=_blank]

When you ran the `quarkus build` command, this created the BuildConfig you can see here.
In our quarkus settings, we set the deployment strategy to build the image using Docker.
The Dockerfile file from the git repo that we cloned was used for this BuildConfig.

[INFO]
====
A build configuration describes a single build definition and a set of triggers for when a new build is created.
Build configurations are defined by a BuildConfig, which is a REST object that can be used in a POST to the API server to create a new instance.
====

You can read more about BuildConfigs https://docs.openshift.com/container-platform/latest/cicd/builds/understanding-buildconfigs.html[here,window=_blank]

Once the BuildConfig was created, the source-to-image process kicked off a Build of that BuildConfig.
The build is what actually does the work in building and deploying the image.
We started with defining what to be built with the BuildConfig and then actually did the work with the Build.
You can read more about Builds https://docs.openshift.com/container-platform/latest/cicd/builds/understanding-image-builds.html[here,window=_blank]

To look at what the build actually did, click on Builds tab and then into the first Build in the list.

image::rosa-console-builds.png[OpenShift Web Console - Builds,link=rosa-console-builds.png,window=_blank]

On the next screen, explore around.
Look specifically at the YAML definition of the build and the logs to see what the build actually did.
If your build failed for some reason, the logs are a great first place to start to look at to debug what happened.

image::rosa-console-build-logs.png[OpenShift Web Console - Build Logs,link=rosa-console-build-logs.png,window=_blank]

=== Image Deployment

After the image was built, the source-to-image process then deployed the application for us.
You can view the deployment under _Workloads_ \-> _Deployments_, and then click on the Deployment name.

image::rosa-console-deployments.png[OpenShift Web Console - Deployments,link=rosa-console-deployments.png,window=_blank]

Explore around the deployment screen, check out the different tabs, look at the YAML that was created.

image::rosa-console-deployment-yaml.png[OpenShift Web Console - Deployment YAML,link=rosa-console-deployment-yaml.png,window=_blank]

Look at the pod the deployment created, and see that it is running.

image::rosa-console-deployment-pods.png[OpenShift Web Console - Deployment Pods,link=rosa-console-deployment-pods.png,window=_blank]

The last thing we will look at is the route that was created for our application.
In the quarkus properties file, we specified that the application should be exposed to the Internet.
When you create a Route, you have the option to specify a hostname.
To start with, we will just use the default domain that comes with ROSA (`openshiftapps.com` in our case).

You can read more about routes https://docs.openshift.com/container-platform/latest/networking/routes/route-configuration.html[in the Red Hat documentation,window=_blank]

From the OpenShift Web Console menu, click on _Networking_\->__Routes__, and the _microsweeper-appservice_ route.

image::rosa-console-routes.png[OpenShift Web Console - Routes,link=rosa-console-routes.png,window=_blank]

=== Test the application

While in the route section of the OpenShift Web Console, click the URL under _Location_:

image::rosa-console-route-link.png[OpenShift Web Console - Route Link,link=rosa-console-route-link.png,window=_blank]

You can also get the the URL for your application using the command line:

[source,sh,role=execute]
----
echo "http://$(oc -n microsweeper-ex get route microsweeper-appservice -o jsonpath='{.spec.host}')"
----

.Sample Output
[source,text,options=nowrap]
----
http://microsweeper-appservice-microsweeper-ex.apps.rosa-6n4s8.1c1c.p1.openshiftapps.com
----

[WARNING]
====
This is an `http` URL. Some browsers (Chrome) replace `http` with `https` automatically which will result in a application not available error. Either use another browser or fix the URL manually in the URL bar.
====

=== Application IP

Let's take a quick look at what IP the application resolves to.

Back in your bastion VM, run the following command:

[source,sh,role=execute]
----
nslookup $(oc -n microsweeper-ex get route microsweeper-appservice -o jsonpath='{.spec.host}')
----

.Sample Output
[source,text,options=nowrap]
----
Server:		192.168.0.2
Address:	192.168.0.2#53

Non-authoritative answer:
Name:	microsweeper-appservice-microsweeper-ex.apps.rosa-6n4s8.1c1c.p1.openshiftapps.com
Address: 54.185.165.99
Name:	microsweeper-appservice-microsweeper-ex.apps.rosa-6n4s8.1c1c.p1.openshiftapps.com
Address: 54.191.151.187
----

Notice the IP address;
can you guess where it comes from?

It comes from the ROSA Load Balancer.
In this workshop, we are using a public cluster which means the load balancer is exposed to the Internet.
If this was a private cluster, you would have to have connectivity to the VPC ROSA is running on.
This could be via a VPN connection, AWS DirectConnect, or something else.

== Summary

Outstanding! You have successfully deployed a cloud-native application with secure AWS integration using IRSA.

=== What you accomplished

In this lab, you:

* ‚úÖ Created AWS DynamoDB table for application data (demonstrating multi-service architecture)
* ‚úÖ Configured OIDC trust policy for secure service account authentication (showing zero trust implementation)
* ‚úÖ Implemented IAM Roles for Service Accounts (IRSA) eliminating static credentials (proving security best practices)
* ‚úÖ Deployed Quarkus application using Source-to-Image (S2I) build (highlighting developer productivity features)
* ‚úÖ Explored OpenShift BuildConfigs, ImageStreams, and Deployments (understanding platform automation)
* ‚úÖ Exposed application via OpenShift Routes for public access (demonstrating ingress management)
* ‚úÖ Verified application functionality with AWS service integration (proving end-to-end security)

=== Key takeaways for customer conversations

* *Zero static credentials*: Application accesses DynamoDB using temporary AWS STS tokens that automatically rotate, eliminating credential management burden
* *Pod-level identity*: Each Kubernetes service account maps to unique IAM role with least-privilege permissions, enabling granular security policies
* *AWS-native security*: Uses same OIDC federation pattern as EKS, leveraging customers' existing AWS security knowledge and tooling
* *Developer experience*: Developers write standard AWS SDK code without credential handling logic, reducing code complexity
* *Audit trail*: CloudTrail automatically tracks all AWS API calls with pod-level attribution for complete accountability

=== Value demonstration for prospects

When showing this to customers, emphasize:

* *Security risk reduction*: Eliminates static AWS access keys throughout the application stack, preventing credential exposure incidents
* *Compliance wins*: Automatic credential rotation satisfies PCI DSS 8.2.4 requirements for key rotation and eliminates manual rotation processes
* *Operational efficiency*: No credential distribution infrastructure needed‚Äîeliminates Vault clusters, secret management overhead, and key distribution systems
* *Cost savings*: Eliminates dedicated secrets management infrastructure and associated operational costs

=== Architecture diagram explanation

The IRSA architecture you implemented follows this flow:

1. *Pod requests AWS access* ‚Üí ServiceAccount annotated with IAM role ARN
2. *ROSA injects OIDC token* ‚Üí Kubernetes token signed by cluster's OIDC provider
3. *AWS STS validates token* ‚Üí Verifies token signature against OIDC provider
4. *Temporary credentials issued* ‚Üí 1-hour TTL credentials with scoped IAM permissions
5. *Application uses credentials* ‚Üí AWS SDK automatically handles token refresh

This architecture provides:

* *Zero long-lived credentials* in cluster or application code
* *Automatic credential rotation* every hour without application restarts
* *Least-privilege access* at pod granularity, not cluster-wide

=== Technical deep dive points

For technical discussions, highlight:

* *OIDC federation*: Same security model as AWS EKS, proving Red Hat uses AWS best practices
* *Service account projection*: Kubernetes-native credential injection without sidecars or init containers
* *IAM trust policy*: Precise control over which namespaces and service accounts can assume roles
* *SDK compatibility*: Works with all AWS SDKs (Java, Python, Go, .NET) without code changes

=== Next steps

Your application is now running with secure AWS DynamoDB integration.

In the next lab, you will implement AWS Secrets Manager to externalize application configuration and secrets, completing the defense-in-depth security strategy.